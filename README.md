# Enhancing-Caption-Diversity-and-Distinctiveness# Enhancing Caption Diversity and Distinctiveness

This project presents an advanced **Transformer-based Image Captioning System** designed to generate **diverse, distinctive, and context-aware captions** for images.
The system is built using **BLIP (Bootstrapped Language Image Pretraining)** and integrates **Vision Transformers (ViT)** with **Transformer-based text decoders**, enhanced by diversity-promoting strategies such as **Top-k / Top-p sampling**, **contrastive learning**, and **reinforcement-based diversity rewards**.

---

## ğŸ“Œ Project Overview

Traditional image captioning systems often generate **generic and repetitive captions**.
This project addresses that limitation by focusing on:

* Caption **diversity**
* Caption **distinctiveness**
* Contextual and semantic richness
* Human-like language generation

A **Flask-based web application** is provided to allow users to upload images and instantly generate enhanced captions with quality evaluation.

---

## ğŸ§  Key Features

* ğŸ” **Transformer-based Image Captioning (BLIP)**
* ğŸ¨ **Multiple caption generation modes**

  * Accurate
  * Creative
  * Diverse
* ğŸ” **Top-k and Top-p (nucleus) sampling**
* ğŸ“‰ **Contrastive learning** for reducing repetitive captions
* ğŸ¯ **Reinforcement learning with diversity rewards**
* ğŸ“Š **Evaluation metrics**

  * BLEU
  * METEOR
  * CIDEr
  * SPICE
* ğŸ–¼ï¸ **Web-based interface using Flask**
* âœï¸ **Grammar and readability enhancement using NLP tools**

---

## ğŸ—ï¸ System Architecture

The system consists of:

* **Vision Encoder**: Vision Transformer (ViT)
* **Text Decoder**: Transformer-based language decoder
* **Diversity Controller**: Sampling + reinforcement strategies
* **Evaluation Module**: Caption quality scoring
* **Web Interface**: Flask application for real-time captioning

---

## ğŸ› ï¸ Technologies Used

* **Programming Language**: Python
* **Deep Learning Framework**: PyTorch
* **Transformer Models**: Hugging Face Transformers (BLIP)
* **Web Framework**: Flask
* **Dataset**: Flickr30k
* **Evaluation & NLP**: NLTK, TextBlob
* **IDE**: VS Code

---

## ğŸ“‚ Project Structure

```text
Enhancing-Caption-Diversity-and-Distinctiveness/
â”‚
â”œâ”€â”€ app_flickr.py              # Flask web application
â”œâ”€â”€ start_app.py               # Entry point (run this)
â”œâ”€â”€ train_flickr_model.py      # Full training script
â”œâ”€â”€ simple_train.py            # Simplified training script
â”œâ”€â”€ test_flickr.py             # Quick testing script
â”œâ”€â”€ requirements.txt           # Project dependencies
â”œâ”€â”€ config/
â”‚   â””â”€â”€ caption_config.json    # Caption strategies & settings
â”œâ”€â”€ models/                    # Saved models (ignored in GitHub)
â”œâ”€â”€ uploads/                   # Uploaded images (ignored)
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## ğŸš€ How to Run the Project

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/Enhancing-Caption-Diversity-and-Distinctiveness.git
cd Enhancing-Caption-Diversity-and-Distinctiveness
```

### 2ï¸âƒ£ Create & Activate Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate   # Windows
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run the Application

```bash
python start_app.py
```

### 5ï¸âƒ£ Open in Browser

```
http://localhost:3000
```

---

## ğŸ–¼ï¸ Sample Output (Screenshots)

> ğŸ“Œ **Add screenshots here after running the project**

```text
ğŸ“· Screenshot 1: Image upload interface
ğŸ“· Screenshot 2: Generated captions with quality scores
```

*(You can add images using: `![Screenshot](screenshots/example.png)`)*

---

## ğŸ“Š Experimental Results

| Metric | Baseline (CNN-LSTM) | Proposed BLIP Model |
| ------ | ------------------- | ------------------- |
| BLEU-1 | 0.58                | 0.74                |
| BLEU-4 | 0.32                | 0.54                |
| METEOR | 0.27                | 0.41                |
| CIDEr  | 0.88                | 1.35                |
| SPICE  | 0.18                | 0.29                |

âœ” The proposed model significantly improves **fluency, diversity, and contextual relevance**.

---

## ğŸ¯ Applications

* Assistive technology for visually impaired users
* Digital media and content automation
* AI-powered storytelling
* E-commerce image description
* Intelligent visual understanding systems

---

## ğŸ”® Future Enhancements

* Multilingual caption generation
* Visual grounding with object detection
* Deployment on mobile and edge devices
* Human-in-the-loop caption refinement
* Bias and fairness analysis

---

## ğŸ‘¨â€ğŸ“ Author

**Karnati Mohith Reddy**
B.Tech â€“ Computer Science and Engineering
Anurag University

---

## ğŸ“œ License

This project is licensed under the **Apache License 2.0**.

---

â­ If you like this project, feel free to star the repository!
